{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f887c3",
   "metadata": {
    "id": "d0f887c3"
   },
   "outputs": [],
   "source": [
    "# This mounts your Google Drive to the Colab VM.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
    "# assignment folder, e.g. 'cs231n/assignments/assignment1/'\n",
    "FOLDERNAME = 'cs231n/assignments/assignment1/'\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# Now that we've mounted your Drive, this ensures that\n",
    "# the Python interpreter of the Colab VM can load\n",
    "# python files from within it.\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "\n",
    "# This downloads the CIFAR-10 dataset to your Drive\n",
    "# if it doesn't already exist.\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
    "!bash get_datasets.sh\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e370a238",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ],
    "id": "e370a238"
   },
   "source": "# 全连接神经网络\n在这个练习中，我们将使用模块化方法实现全连接网络。对于每一层，我们将实现一个 `forward` 和一个 `backward` 函数。`forward` 函数将接收输入、权重和其他参数，并返回一个输出和一个 `cache` 对象，如下所示：\n\n```python\ndef layer_forward(x, w):\n  \"\"\" 接收输入 x 和权重 w \"\"\"\n  # 进行一些计算 ...\n  z = # ... 一些中间值\n  # 进行更多计算 ...\n  out = # 输出\n   \n  cache = (x, w, z, out) # 我们需要计算梯度的值\n   \n  return out, cache\n```\n\nbackward pass 将接收上游导数和 `cache` 对象，并返回关于输入和权重的梯度，如下所示：\n\n```python\ndef layer_backward(dout, cache):\n  \"\"\"\n  接收 dout（损失对输出的导数）和 cache，\n  并计算对输入的导数。\n  \"\"\"\n  # 解包缓存值\n  x, w, z, out = cache\n  \n  # 使用缓存中的值计算导数\n  dx = # 损失对 x 的导数\n  dw = # 损失对 w 的导数\n  \n  return dx, dw\n```\n\n在以这种方式实现大量层之后，我们将能够轻松地将它们组合起来构建具有不同架构的分类器。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acf2688",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ],
    "id": "0acf2688"
   },
   "outputs": [],
   "source": "# 像往常一样，一些设置代码\nfrom __future__ import print_function\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom cs231n.classifiers.fc_net import *\nfrom cs231n.data_utils import get_CIFAR10_data\nfrom cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\nfrom cs231n.solver import Solver\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (10.0, 8.0) # 设置绘图的默认大小\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n# 用于自动重新加载外部模块\n# 参见 http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n%load_ext autoreload\n%autoreload 2\n\ndef rel_error(x, y):\n  \"\"\" 返回相对误差 \"\"\"\n  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909acc7b",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ],
    "id": "909acc7b"
   },
   "outputs": [],
   "source": "# 加载（预处理的）CIFAR10 数据。\n\ndata = get_CIFAR10_data()\nfor k, v in list(data.items()):\n  print(('%s: ' % k, v.shape))"
  },
  {
   "cell_type": "markdown",
   "id": "e7851b32",
   "metadata": {
    "id": "e7851b32"
   },
   "source": "# 仿射层：前向传播\n打开文件 `cs231n/layers.py` 并实现 `affine_forward` 函数。\n\n完成后，你可以通过运行以下命令测试你的实现："
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9408a08c",
   "metadata": {
    "id": "9408a08c"
   },
   "outputs": [],
   "source": "# 测试 affine_forward 函数\n\nnum_inputs = 2\ninput_shape = (4, 5, 6)\noutput_dim = 3\n\ninput_size = num_inputs * np.prod(input_shape)\nweight_size = output_dim * np.prod(input_shape)\n\nx = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\nw = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\nb = np.linspace(-0.3, 0.1, num=output_dim)\n\nout, _ = affine_forward(x, w, b)\ncorrect_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n                        [ 3.25553199,  3.5141327,   3.77273342]])\n\n# 将你的输出与我们的进行比较。误差应该在 e-9 左右或更小。\nprint('测试 affine_forward 函数：')\nprint('差异: ', rel_error(out, correct_out))"
  },
  {
   "cell_type": "markdown",
   "id": "bfdfab60",
   "metadata": {
    "id": "bfdfab60"
   },
   "source": "# 仿射层：反向传播\n现在实现 `affine_backward` 函数，并使用数值梯度检查测试你的实现。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ebdc09",
   "metadata": {
    "id": "42ebdc09"
   },
   "outputs": [],
   "source": "# 测试 affine_backward 函数\nnp.random.seed(231)\nx = np.random.randn(10, 2, 3)\nw = np.random.randn(6, 5)\nb = np.random.randn(5)\ndout = np.random.randn(10, 5)\n\ndx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\ndw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\ndb_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n\n_, cache = affine_forward(x, w, b)\ndx, dw, db = affine_backward(dout, cache)\n\n# 误差应该在 e-10 左右或更小\nprint('测试 affine_backward 函数：')\nprint('dx 误差: ', rel_error(dx_num, dx))\nprint('dw 误差: ', rel_error(dw_num, dw))\nprint('db 误差: ', rel_error(db_num, db))"
  },
  {
   "cell_type": "markdown",
   "id": "00b019e3",
   "metadata": {
    "id": "00b019e3"
   },
   "source": "# ReLU 激活：前向传播\n在 `relu_forward` 函数中实现 ReLU 激活函数的前向传播，并使用以下代码测试你的实现："
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007bbf68",
   "metadata": {
    "id": "007bbf68"
   },
   "outputs": [],
   "source": "# 测试 relu_forward 函数\n\nx = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n\nout, _ = relu_forward(x)\ncorrect_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n                        [ 0.,          0.,          0.04545455,  0.13636364,],\n                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n\n# 将你的输出与我们的进行比较。误差应该在 e-8 左右\nprint('测试 relu_forward 函数：')\nprint('差异: ', rel_error(out, correct_out))"
  },
  {
   "cell_type": "markdown",
   "id": "3ac8a267",
   "metadata": {
    "id": "3ac8a267"
   },
   "source": "# ReLU 激活：反向传播\n现在在 `relu_backward` 函数中实现 ReLU 激活函数的反向传播，并使用数值梯度检查测试你的实现："
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a1bb2",
   "metadata": {
    "id": "304a1bb2"
   },
   "outputs": [],
   "source": "np.random.seed(231)\nx = np.random.randn(10, 10)\ndout = np.random.randn(*x.shape)\n\ndx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n\n_, cache = relu_forward(x)\ndx = relu_backward(dout, cache)\n\n# 误差应该在 e-12 左右\nprint('测试 relu_backward 函数：')\nprint('dx 误差: ', rel_error(dx_num, dx))"
  },
  {
   "cell_type": "markdown",
   "id": "e17b23f3",
   "metadata": {
    "tags": [
     "pdf-inline"
    ],
    "id": "e17b23f3"
   },
   "source": "## 内联问题 1：\n\n我们只要求你实现 ReLU，但神经网络中可以使用许多不同的激活函数，每个都有其优缺点。特别是，激活函数的一个常见问题是在反向传播期间获得零（或接近零）的梯度流。以下哪些激活函数有这个问题？如果你在一维情况下考虑这些函数，什么类型的输入会导致这种行为？\n1. Sigmoid\n2. ReLU\n3. Leaky ReLU\n\n$\\color{blue}{\\textit 你的答案：}$ *在此填写*"
  },
  {
   "cell_type": "markdown",
   "id": "e9daf016",
   "metadata": {
    "id": "e9daf016"
   },
   "source": "# \"三明治\"层\n有一些在神经网络中经常使用的层模式。例如，仿射层之后经常跟着 ReLU 非线性。为了使这些常见模式容易使用，我们在文件 `cs231n/layer_utils.py` 中定义了几个便利层。\n\n现在请查看 `affine_relu_forward` 和 `affine_relu_backward` 函数，并运行以下代码进行数值梯度检查："
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30088927",
   "metadata": {
    "id": "30088927"
   },
   "outputs": [],
   "source": "from cs231n.layer_utils import affine_relu_forward, affine_relu_backward\nnp.random.seed(231)\nx = np.random.randn(2, 3, 4)\nw = np.random.randn(12, 10)\nb = np.random.randn(10)\ndout = np.random.randn(2, 10)\n\nout, cache = affine_relu_forward(x, w, b)\ndx, dw, db = affine_relu_backward(dout, cache)\n\ndx_num = eval_numerical_gradient_array(lambda x: affine_relu_forward(x, w, b)[0], x, dout)\ndw_num = eval_numerical_gradient_array(lambda w: affine_relu_forward(x, w, b)[0], w, dout)\ndb_num = eval_numerical_gradient_array(lambda b: affine_relu_forward(x, w, b)[0], b, dout)\n\n# 相对误差应该在 e-10 左右或更小\nprint('测试 affine_relu_forward 和 affine_relu_backward：')\nprint('dx 误差: ', rel_error(dx_num, dx))\nprint('dw 误差: ', rel_error(dw_num, dw))\nprint('db 误差: ', rel_error(db_num, db))"
  },
  {
   "cell_type": "markdown",
   "id": "37dd603b",
   "metadata": {
    "id": "37dd603b"
   },
   "source": "# 损失层：Softmax\n现在在 `cs231n/layers.py` 的 `softmax_loss` 函数中实现 Softmax 的损失和梯度。这些应该类似于你在 `cs231n/classifiers/softmax.py` 中实现的。其他损失函数（例如 `svm_loss`）也可以以模块化方式实现，但是本作业不需要。\n\n你可以通过运行以下代码确保实现正确："
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d5269d",
   "metadata": {
    "id": "22d5269d"
   },
   "outputs": [],
   "source": "np.random.seed(231)\nnum_classes, num_inputs = 10, 50\nx = 0.001 * np.random.randn(num_inputs, num_classes)\ny = np.random.randint(num_classes, size=num_inputs)\n\n\ndx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\nloss, dx = softmax_loss(x, y)\n\n# 测试 softmax_loss 函数。损失应该接近 2.3，dx 误差应该在 e-8 左右\nprint('\\n测试 softmax_loss：')\nprint('损失: ', loss)\nprint('dx 误差: ', rel_error(dx_num, dx))"
  },
  {
   "cell_type": "markdown",
   "id": "5898b95a",
   "metadata": {
    "id": "5898b95a"
   },
   "source": "# 两层网络\n打开文件 `cs231n/classifiers/fc_net.py` 并完成 `TwoLayerNet` 类的实现。通读它以确保你理解 API。你可以运行下面的单元格来测试你的实现。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a83db9b",
   "metadata": {
    "id": "0a83db9b"
   },
   "outputs": [],
   "source": "np.random.seed(231)\nN, D, H, C = 3, 5, 50, 7\nX = np.random.randn(N, D)\ny = np.random.randint(C, size=N)\n\nstd = 1e-3\nmodel = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)\n\nprint('测试初始化 ... ')\nW1_std = abs(model.params['W1'].std() - std)\nb1 = model.params['b1']\nW2_std = abs(model.params['W2'].std() - std)\nb2 = model.params['b2']\nassert W1_std < std / 10, '第一层权重看起来不对'\nassert np.all(b1 == 0), '第一层偏置看起来不对'\nassert W2_std < std / 10, '第二层权重看起来不对'\nassert np.all(b2 == 0), '第二层偏置看起来不对'\n\nprint('测试推理时前向传播 ... ')\nmodel.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\nmodel.params['b1'] = np.linspace(-0.1, 0.9, num=H)\nmodel.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\nmodel.params['b2'] = np.linspace(-0.9, 0.1, num=C)\nX = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\nscores = model.loss(X)\ncorrect_scores = np.asarray(\n  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\nscores_diff = np.abs(scores - correct_scores).sum()\nassert scores_diff < 1e-6, '推理时前向传播出现问题'\n\nprint('测试训练时损失（无正则化）')\ny = np.asarray([0, 5, 1])\nloss, grads = model.loss(X, y)\ncorrect_loss = 3.4702243556\nassert abs(loss - correct_loss) < 1e-10, '训练时损失出现问题'\n\nmodel.reg = 1.0\nloss, grads = model.loss(X, y)\ncorrect_loss = 26.5948426952\nassert abs(loss - correct_loss) < 1e-10, '正则化损失出现问题'\n\n# 误差应该在 e-7 左右或更小\nfor reg in [0.0, 0.7]:\n  print('运行数值梯度检查与 reg = ', reg)\n  model.reg = reg\n  loss, grads = model.loss(X, y)\n\n  for name in sorted(grads):\n    f = lambda _: model.loss(X, y)[0]\n    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n    print('%s 相对误差: %.2e' % (name, rel_error(grad_num, grads[name])))"
  },
  {
   "cell_type": "markdown",
   "id": "0adf7b47",
   "metadata": {
    "id": "0adf7b47"
   },
   "source": "# 求解器\n打开文件 `cs231n/solver.py` 并通读它以熟悉 API。完成后，使用 `Solver` 实例训练一个 `TwoLayerNet`，在验证集上达到大约 `36%` 的准确率。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13472b96",
   "metadata": {
    "id": "13472b96"
   },
   "outputs": [],
   "source": "input_size = 32 * 32 * 3\nhidden_size = 50\nnum_classes = 10\nmodel = TwoLayerNet(input_size, hidden_size, num_classes)\nsolver = None\n\n##############################################################################\n# TODO: 使用 Solver 实例训练一个 TwoLayerNet，在验证集上达到大约 36% #\n# 的准确率。                                                                #\n##############################################################################\n\n##############################################################################\n#                             END OF YOUR CODE                               #\n##############################################################################"
  },
  {
   "cell_type": "markdown",
   "id": "ffdfc7f4",
   "metadata": {
    "id": "ffdfc7f4"
   },
   "source": "# 调试训练\n使用我们上面提供的默认参数，你应该得到验证集上约 0.36 的验证准确率。这不是很好。\n\n获得对问题所在深入了解的一个策略是绘制优化期间的损失函数以及训练和验证集上的准确率。\n\n另一个策略是可视化网络第一层学到的权重。在大多数在视觉数据上训练的神经网络中，可视化时第一层权重通常显示出一些可见的结构。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3649ea",
   "metadata": {
    "id": "1a3649ea"
   },
   "outputs": [],
   "source": "# 运行此单元格以可视化训练损失和训练/验证准确率\n\nplt.subplot(2, 1, 1)\nplt.title('训练损失')\nplt.plot(solver.loss_history, 'o')\nplt.xlabel('迭代')\n\nplt.subplot(2, 1, 2)\nplt.title('准确率')\nplt.plot(solver.train_acc_history, '-o', label='训练')\nplt.plot(solver.val_acc_history, '-o', label='验证')\nplt.plot([0.5] * len(solver.val_acc_history), 'k--')\nplt.xlabel('轮次')\nplt.legend(loc='lower right')\nplt.gcf().set_size_inches(15, 12)\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6052910",
   "metadata": {
    "id": "a6052910"
   },
   "outputs": [],
   "source": "from cs231n.vis_utils import visualize_grid\n\n# 可视化网络权重\n\ndef show_net_weights(net):\n    W1 = net.params['W1']\n    W1 = W1.reshape(3, 32, 32, -1).transpose(3, 1, 2, 0)\n    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n    plt.gca().axis('off')\n    plt.show()\n\nshow_net_weights(model)"
  },
  {
   "cell_type": "markdown",
   "id": "68386e68",
   "metadata": {
    "id": "68386e68"
   },
   "source": "# 调整你的超参数\n\n**出了什么问题？**。查看上面的可视化，我们看到损失大体上线性下降，这似乎表明学习率可能太低。此外，训练和验证准确率之间没有差距，这表明我们使用的模型容量较低，我们应该增加其大小。另一方面，使用非常大的模型，我们期望看到更多过拟合，这表现为训练和验证准确率之间存在很大差距。\n\n**调整**。调整超参数并培养它们如何影响最终性能的直觉是使用神经网络的重要组成部分，所以我们希望你得到大量练习。在下面，你应该尝试不同的各种超参数值，包括隐藏层大小、学习率、训练轮数和正则化强度。你也可以考虑调整学习率衰减，但使用默认值你应该能够获得良好的性能。\n\n**近似结果**。你应该在验证集上实现超过 48% 的分类准确率。我们的最佳网络在验证集上得到超过 52%。\n\n**实验**：你在这个练习中的目标是在 CIFAR-10 上获得尽可能好的结果（52% 可以作为参考），使用全连接神经网络。随意实现你自己的技术（例如 PCA 降低维度，或添加 dropout，或向求解器添加功能等）。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d308eb5",
   "metadata": {
    "scrolled": false,
    "id": "3d308eb5"
   },
   "outputs": [],
   "source": "best_model = None\n\n\n\n#################################################################################\n# TODO: 使用验证集调整超参数。将你训练的最佳模型存储在\n# best_model 中。                                                                #\n#                                                                               #\n# 为了帮助你调试网络，使用类似于我们上面使用的可视化可能会有帮助；这些可视化与我们上面为调整不佳的网络看到的会有显著的定性差异。    #\n#                                                                               #\n# 手动调整超参数很有趣，但你可能会发现编写代码来自动遍历超参数的组合            #\n# 组合会很有用，就像我们之前做的那样。                                         #\n#################################################################################\n\n################################################################################\n#                              END OF YOUR CODE                                #\n################################################################################"
  },
  {
   "cell_type": "markdown",
   "id": "6c70f18f",
   "metadata": {
    "id": "6c70f18f"
   },
   "source": "# 测试你的模型！\n运行你的最佳模型在验证和测试集上。你应该在验证集和测试集上实现超过 48% 的准确率。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666e7589",
   "metadata": {
    "test": "val_accuracy",
    "id": "666e7589"
   },
   "outputs": [],
   "source": [
    "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n",
    "print('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d74b7b6",
   "metadata": {
    "test": "test_accuracy",
    "id": "0d74b7b6"
   },
   "outputs": [],
   "source": "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\nprint('测试集准确率: ', (y_test_pred == data['y_test']).mean())"
  },
  {
   "cell_type": "code",
   "source": "# 保存最佳模型\nbest_model.save(\"best_two_layer_net.npy\")",
   "metadata": {
    "id": "JlZg7TkKfesG"
   },
   "id": "JlZg7TkKfesG",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ab8e2ae9",
   "metadata": {
    "id": "ab8e2ae9"
   },
   "source": "## 内联问题 2：\n\n现在你已经训练了一个神经网络分类器，你可能会发现测试准确率远低于训练准确率。我们可以通过哪些方式减少这种差距？选择所有适用的选项。\n\n1. 在更大的数据集上训练。\n2. 添加更多隐藏单元。\n3. 增加正则化强度。\n4. 以上都不对。\n\n$\\color{blue}{\\textit 你的答案：}$\n\n$\\color{blue}{\\textit 你的解释：}$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af96f6b3",
   "metadata": {
    "id": "af96f6b3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}