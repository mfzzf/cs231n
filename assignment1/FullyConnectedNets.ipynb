{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_Ke3JSAu-pN"
   },
   "outputs": [],
   "source": [
    "# This mounts your Google Drive to the Colab VM.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
    "# assignment folder, e.g. 'cs231n/assignments/assignment1/'\n",
    "FOLDERNAME = 'cs231n/assignments/assignment1/'\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# Now that we've mounted your Drive, this ensures that\n",
    "# the Python interpreter of the Colab VM can load\n",
    "# python files from within it.\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "\n",
    "# This downloads the CIFAR-10 dataset to your Drive\n",
    "# if it doesn't already exist.\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
    "# %cd /content/drive/My\\ Drive/$FOLDERNAME\n",
    "!bash get_datasets.sh\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hmW4qyEu-pR"
   },
   "source": "# 多层全连接网络\n在这个练习中，你将实现一个具有任意数量隐藏层的全连接网络。"
  },
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "Qn88dLWLA3np"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "audT-ccNu-pT"
   },
   "source": "阅读 `cs231n/classifiers/fc_net.py` 文件中的 `FullyConnectedNet` 类。\n\n实现网络初始化、前向传播和反向传播。在整个作业中，你将在 `cs231n/layers.py` 中实现层。你可以重新使用之前实现的 `affine_forward`、`affine_backward`、`relu_forward`、`relu_backward` 和 `softmax_loss`。现在不用担心实现 dropout 或批/层归一化，稍后你会添加这些功能。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ],
    "id": "vYqS6VtUu-pU"
   },
   "outputs": [],
   "source": "# 设置单元格。\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom cs231n.classifiers.fc_net import *\nfrom cs231n.data_utils import get_CIFAR10_data\nfrom cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\nfrom cs231n.solver import Solver\n\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # 设置绘图的默认大小。\nplt.rcParams[\"image.interpolation\"] = \"nearest\"\nplt.rcParams[\"image.cmap\"] = \"gray\"\n\n%load_ext autoreload\n%autoreload 2\n\ndef rel_error(x, y):\n    \"\"\"返回相对误差。\"\"\"\n    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56aAEw7Ku-pU"
   },
   "outputs": [],
   "source": "# 加载（预处理的）CIFAR-10 数据。\ndata = get_CIFAR10_data()\nfor k, v in list(data.items()):\n    print(f\"{k}: {v.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L60FVoXSu-pV"
   },
   "source": "## 初始损失和梯度检查\n\n作为健全性检查，运行以下代码检查初始损失并进行网络梯度检查（无论是否有正则化）。这是查看初始损失是否合理的好方法。\n\n对于梯度检查，你应该期望看到大约 1e-7 或更小的误差。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffLH91h4u-pV"
   },
   "outputs": [],
   "source": "np.random.seed(231)\nN, D, H1, H2, C = 2, 15, 20, 30, 10\nX = np.random.randn(N, D)\ny = np.random.randint(C, size=(N,))\n\nfor reg in [0, 3.14]:\n    print(\"运行检查与 reg = \", reg)\n    model = FullyConnectedNet(\n        [H1, H2],\n        input_dim=D,\n        num_classes=C,\n        reg=reg,\n        weight_scale=5e-2,\n        dtype=np.float64\n    )\n\n    loss, grads = model.loss(X, y)\n    print(\"初始损失: \", loss)\n\n    # 大多数误差应该在 e-7 或更小的数量级。\n    # 注意：当 reg = 0.0 时，看到 W2 的 e-5 数量级的误差也可以\n    for name in sorted(grads):\n        f = lambda _: model.loss(X, y)[0]\n        grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n        print(f\"{name} 相对误差: {rel_error(grad_num, grads[name])}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSiVnvNgu-pW"
   },
   "source": [
    "As another sanity check, make sure your network can overfit on a small dataset of 50 images. First, we will try a three-layer network with 100 units in each hidden layer. In the following cell, tweak the **learning rate** and **weight initialization scale** to overfit and achieve 100% training accuracy within 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "id": "HYHs3IPmu-pW"
   },
   "outputs": [],
   "source": "# TODO: 使用三层网络通过调整学习率和初始化尺度来过拟合 50 个训练样本。\n\nnum_train = 50\nsmall_data = {\n  \"X_train\": data[\"X_train\"][:num_train],\n  \"y_train\": data[\"y_train\"][:num_train],\n  \"X_val\": data[\"X_val\"],\n  \"y_val\": data[\"y_val\"],\n}\n\nweight_scale = 1e-2   # 尝试这个！\nlearning_rate = 1e-4  # 尝试这个！\n\n\nmodel = FullyConnectedNet(\n    [100, 100],\n    weight_scale=weight_scale,\n    dtype=np.float64\n)\nsolver = Solver(\n    model,\n    small_data,\n    print_every=10,\n    num_epochs=20,\n    batch_size=25,\n    update_rule=\"sgd\",\n    optim_config={\"learning_rate\": learning_rate},\n)\nsolver.train()\n\nplt.plot(solver.loss_history)\nplt.title(\"训练损失历史\")\nplt.xlabel(\"迭代\")\nplt.ylabel(\"训练损失\")\nplt.grid(linestyle='--', linewidth=0.5)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJHKHQUIu-pX"
   },
   "source": [
    "Now, try to use a five-layer network with 100 units on each layer to overfit on 50 training examples. Again, you will have to adjust the learning rate and weight initialization scale, but you should be able to achieve 100% training accuracy within 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vbp8Nsayu-pX"
   },
   "outputs": [],
   "source": "# TODO: 使用五层网络通过调整学习率和初始化尺度来过拟合 50 个训练样本。\n\nnum_train = 50\nsmall_data = {\n  'X_train': data['X_train'][:num_train],\n  'y_train': data['y_train'][:num_train],\n  'X_val': data['X_val'],\n  'y_val': data['y_val'],\n}\n\nlearning_rate = 2e-3  # 尝试这个！\nweight_scale = 1e-5   # 尝试这个！\n\n\nmodel = FullyConnectedNet(\n    [100, 100, 100, 100],\n    weight_scale=weight_scale,\n    dtype=np.float64\n)\nsolver = Solver(\n    model,\n    small_data,\n    print_every=10,\n    num_epochs=20,\n    batch_size=25,\n    update_rule='sgd',\n    optim_config={'learning_rate': learning_rate},\n)\nsolver.train()\n\nplt.plot(solver.loss_history)\nplt.title('训练损失历史')\nplt.xlabel('迭代')\nplt.ylabel('训练损失')\nplt.grid(linestyle='--', linewidth=0.5)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ],
    "id": "sOdiuVmCu-pX"
   },
   "source": "## 内联问题 1：\n你有没有注意到训练三层网络与训练五层网络的相对难度有什么特别之处？特别是，根据你的经验，哪个网络对初始化尺度更敏感？你认为这是为什么？\n\n## 答案：\n[填写此处]"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWe_zEgFu-pY"
   },
   "source": "# 更新规则\n到目前为止，我们使用 vanilla 随机梯度下降（SGD）作为我们的更新规则。更复杂的更新规则可以使训练深度网络更容易。我们将实现几种最常用的更新规则，并将它们与 vanilla SGD 进行比较。"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9l25b5jAu-pY"
   },
   "source": "## SGD+动量\n带动量的随机梯度下降是一种广泛使用的更新规则，它往往使深度网络比 vanilla 随机梯度下降收敛更快。更多信息请参见 http://cs231n.github.io/neural-networks-3/#sgd 的动量更新部分。\n\n打开文件 `cs231n/optim.py` 并阅读文件顶部的文档，以确保你理解 API。在函数 `sgd_momentum` 中实现 SGD+动量更新规则，并运行以下代码检查你的实现。你应该看到小于 e-8 的误差。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fR5m7nuXu-pY"
   },
   "outputs": [],
   "source": "from cs231n.optim import sgd_momentum\n\nN, D = 4, 5\nw = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\ndw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\nv = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n\nconfig = {\"learning_rate\": 1e-3, \"velocity\": v}\nnext_w, _ = sgd_momentum(w, dw, config=config)\n\nexpected_next_w = np.asarray([\n  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])\nexpected_velocity = np.asarray([\n  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])\n\n# 应该看到 e-8 或更小的相对误差\nprint(\"next_w 误差: \", rel_error(next_w, expected_next_w))\nprint(\"velocity 误差: \", rel_error(expected_velocity, config[\"velocity\"]))"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3A5WZm78u-pY"
   },
   "source": [
    "Once you have done so, run the following to train a six-layer network with both SGD and SGD+momentum. You should see the SGD+momentum update rule converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "id": "z66zjXhCu-pZ"
   },
   "outputs": [],
   "source": [
    "num_train = 4000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "solvers = {}\n",
    "\n",
    "for update_rule in ['sgd', 'sgd_momentum']:\n",
    "    print('Running with ', update_rule)\n",
    "    model = FullyConnectedNet(\n",
    "        [100, 100, 100, 100, 100],\n",
    "        weight_scale=5e-2\n",
    "    )\n",
    "\n",
    "    solver = Solver(\n",
    "        model,\n",
    "        small_data,\n",
    "        num_epochs=5,\n",
    "        batch_size=100,\n",
    "        update_rule=update_rule,\n",
    "        optim_config={'learning_rate': 5e-3},\n",
    "        verbose=True,\n",
    "    )\n",
    "    solvers[update_rule] = solver\n",
    "    solver.train()\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 15))\n",
    "\n",
    "axes[0].set_title('Training loss')\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[1].set_title('Training accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[2].set_title('Validation accuracy')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in solvers.items():\n",
    "    axes[0].plot(solver.loss_history, label=f\"loss_{update_rule}\")\n",
    "    axes[1].plot(solver.train_acc_history, label=f\"train_acc_{update_rule}\")\n",
    "    axes[2].plot(solver.val_acc_history, label=f\"val_acc_{update_rule}\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.legend(loc=\"best\", ncol=4)\n",
    "    ax.grid(linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0EQBM5xu-pZ"
   },
   "source": [
    "## RMSProp and Adam\n",
    "RMSProp [1] and Adam [2] are update rules that set per-parameter learning rates by using a running average of the second moments of gradients.\n",
    "\n",
    "In the file `cs231n/optim.py`, implement the RMSProp update rule in the `rmsprop` function and implement the Adam update rule in the `adam` function, and check your implementations using the tests below.\n",
    "\n",
    "**NOTE:** Please implement the _complete_ Adam update rule (with the bias correction mechanism), not the first simplified version mentioned in the course notes.\n",
    "\n",
    "[1] Tijmen Tieleman and Geoffrey Hinton. \"Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.\" COURSERA: Neural Networks for Machine Learning 4 (2012).\n",
    "\n",
    "[2] Diederik Kingma and Jimmy Ba, \"Adam: A Method for Stochastic Optimization\", ICLR 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1w_hTsl-u-pZ"
   },
   "outputs": [],
   "source": [
    "# Test RMSProp implementation\n",
    "from cs231n.optim import rmsprop\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "cache = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'cache': cache}\n",
    "next_w, _ = rmsprop(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.39223849, -0.34037513, -0.28849239, -0.23659121, -0.18467247],\n",
    "  [-0.132737,   -0.08078555, -0.02881884,  0.02316247,  0.07515774],\n",
    "  [ 0.12716641,  0.17918792,  0.23122175,  0.28326742,  0.33532447],\n",
    "  [ 0.38739248,  0.43947102,  0.49155973,  0.54365823,  0.59576619]])\n",
    "expected_cache = np.asarray([\n",
    "  [ 0.5976,      0.6126277,   0.6277108,   0.64284931,  0.65804321],\n",
    "  [ 0.67329252,  0.68859723,  0.70395734,  0.71937285,  0.73484377],\n",
    "  [ 0.75037008,  0.7659518,   0.78158892,  0.79728144,  0.81302936],\n",
    "  [ 0.82883269,  0.84469141,  0.86060554,  0.87657507,  0.8926    ]])\n",
    "\n",
    "# You should see relative errors around e-7 or less\n",
    "print('next_w error: ', rel_error(expected_next_w, next_w))\n",
    "print('cache error: ', rel_error(expected_cache, config['cache']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s96oB_Lmu-pa"
   },
   "outputs": [],
   "source": [
    "# Test Adam implementation\n",
    "from cs231n.optim import adam\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "m = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'm': m, 'v': v, 't': 5}\n",
    "next_w, _ = adam(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n",
    "  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n",
    "  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n",
    "  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])\n",
    "expected_v = np.asarray([\n",
    "  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n",
    "  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n",
    "  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n",
    "  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]])\n",
    "expected_m = np.asarray([\n",
    "  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n",
    "  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n",
    "  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n",
    "  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])\n",
    "\n",
    "# You should see relative errors around e-7 or less\n",
    "print('next_w error: ', rel_error(expected_next_w, next_w))\n",
    "print('v error: ', rel_error(expected_v, config['v']))\n",
    "print('m error: ', rel_error(expected_m, config['m']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DB7lZjH9u-pa"
   },
   "source": [
    "Once you have debugged your RMSProp and Adam implementations, run the following to train a pair of deep networks using these new update rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "id": "Otf_c60Qu-pa"
   },
   "outputs": [],
   "source": [
    "learning_rates = {'rmsprop': 1e-4, 'adam': 1e-3}\n",
    "for update_rule in ['adam', 'rmsprop']:\n",
    "    print('Running with ', update_rule)\n",
    "    model = FullyConnectedNet(\n",
    "        [100, 100, 100, 100, 100],\n",
    "        weight_scale=5e-2\n",
    "    )\n",
    "    solver = Solver(\n",
    "        model,\n",
    "        small_data,\n",
    "        num_epochs=5,\n",
    "        batch_size=100,\n",
    "        update_rule=update_rule,\n",
    "        optim_config={'learning_rate': learning_rates[update_rule]},\n",
    "        verbose=True\n",
    "    )\n",
    "    solvers[update_rule] = solver\n",
    "    solver.train()\n",
    "    print()\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 15))\n",
    "\n",
    "axes[0].set_title('Training loss')\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[1].set_title('Training accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[2].set_title('Validation accuracy')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in solvers.items():\n",
    "    axes[0].plot(solver.loss_history, label=f\"{update_rule}\")\n",
    "    axes[1].plot(solver.train_acc_history, label=f\"{update_rule}\")\n",
    "    axes[2].plot(solver.val_acc_history, label=f\"{update_rule}\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.legend(loc='best', ncol=4)\n",
    "    ax.grid(linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ],
    "id": "iTSO9l58u-pa"
   },
   "source": [
    "## Inline Question 2:\n",
    "\n",
    "AdaGrad, like Adam, is a per-parameter optimization method that uses the following update rule:\n",
    "\n",
    "```\n",
    "cache += dw**2\n",
    "w += - learning_rate * dw / (np.sqrt(cache) + eps)\n",
    "```\n",
    "\n",
    "John notices that when he was training a network with AdaGrad that the updates became very small, and that his network was learning slowly. Using your knowledge of the AdaGrad update rule, why do you think the updates would become very small? Would Adam have the same issue?\n",
    "\n",
    "\n",
    "## Answer:\n",
    "[FILL THIS IN]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUktPczWu-pa"
   },
   "source": "# 训练一个好的模型！\n在 CIFAR-10 上训练你能做到最好的全连接模型，将你最好的模型存储在 `best_model` 变量中。我们要求你在使用全连接网络的验证集上获得至少 50% 的准确率。\n\n如果你小心，应该可以得到超过 55% 的准确率，但我们不要求这个部分，也不为此分配额外学分。在下一个作业中，我们将要求你在 CIFAR-10 上训练你能做到最好的卷积网络，我们希望你把精力花在卷积网络上，而不是全连接网络上。\n\n**注意：** 在下一个作业中，你将学习 BatchNormalization 和 Dropout 等技术，这些可以帮助你训练强大的模型。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "id": "xLiabOb3u-pb"
   },
   "outputs": [],
   "source": "best_model = None\n\n################################################################################\n# TODO: 在 CIFAR-10 上训练你能做到最好的 FullyConnectedNet。你可能会发现批/层归一化和 dropout 有用。将你最好的模型存储在 best_model 变量中。                                                     #\n################################################################################\n# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n\n\n\n# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n################################################################################\n#                              END OF YOUR CODE                                #\n################################################################################"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Dw4p7kIu-pb"
   },
   "source": "# 测试你的模型！\n运行你最好的模型在验证和测试集上。你应该在验证集和测试集上实现至少 50% 的准确率。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "test": "val_test_accuracy",
    "id": "A-mWD7-Su-pb"
   },
   "outputs": [],
   "source": "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\ny_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\nprint('验证集准确率: ', (y_val_pred == data['y_val']).mean())\nprint('测试集准确率: ', (y_test_pred == data['y_test']).mean())"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}