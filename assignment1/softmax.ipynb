{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a1c548",
   "metadata": {
    "id": "a7a1c548"
   },
   "outputs": [],
   "source": [
    "# This mounts your Google Drive to the Colab VM.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
    "# assignment folder, e.g. 'cs231n/assignments/assignment1/'\n",
    "FOLDERNAME = 'cs231n/assignments/assignment1/'\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# Now that we've mounted your Drive, this ensures that\n",
    "# the Python interpreter of the Colab VM can load\n",
    "# python files from within it.\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "\n",
    "# This downloads the CIFAR-10 dataset to your Drive\n",
    "# if it doesn't already exist.\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
    "!bash get_datasets.sh\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08abd1b",
   "metadata": {
    "tags": [
     "pdf-title"
    ],
    "id": "f08abd1b"
   },
   "source": "# Softmax 分类器练习\n\n*请完成并提交这份完整的作业工作表（包括输出和任何工作表之外的辅助代码）。更多详情请查看课程网站上的[作业页面](http://vision.stanford.edu/teaching/cs231n/assignments.html)。*\n\n在这个练习中，你将：\n\n- 实现 Softmax 分类器的完全向量化的**损失函数**\n- 实现其**解析梯度**的完全向量化表达式\n- 使用数值梯度**检查你的实现**\n- 使用验证集来**调整学习率和正则化强度**\n- 使用**SGD**优化损失函数\n- **可视化**最终学到的权重"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3defe387",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ],
    "id": "3defe387"
   },
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook.\n",
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the\n",
    "# notebook rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8603125",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ],
    "id": "b8603125"
   },
   "source": "## CIFAR-10 数据加载和预处理"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf85b0",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ],
    "id": "23bf85b0"
   },
   "outputs": [],
   "source": "# 加载原始 CIFAR-10 数据。\ncifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n\n# 清理变量以防止多次加载数据（可能导致内存问题）\ntry:\n   del X_train, y_train\n   del X_test, y_test\n   print('清除之前加载的数据。')\nexcept:\n   pass\n\nX_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n\n# 作为健全性检查，我们打印出训练和测试数据的大小。\nprint('训练数据维度: ', X_train.shape)\nprint('训练标签维度: ', y_train.shape)\nprint('测试数据维度: ', X_test.shape)\nprint('测试标签维度: ', y_test.shape)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471ea325",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ],
    "id": "471ea325"
   },
   "outputs": [],
   "source": "# 可视化数据集中的一些示例。\n# 我们展示来自每个类别的几个训练图像示例。\nclasses = ['飞机', '汽车', '鸟', '猫', '鹿', '狗', '青蛙', '马', '船', '卡车']\nnum_classes = len(classes)\nsamples_per_class = 7\nfor y, cls in enumerate(classes):\n    idxs = np.flatnonzero(y_train == y)\n    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n    for i, idx in enumerate(idxs):\n        plt_idx = i * num_classes + y + 1\n        plt.subplot(samples_per_class, num_classes, plt_idx)\n        plt.imshow(X_train[idx].astype('uint8'))\n        plt.axis('off')\n        if i == 0:\n            plt.title(cls)\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd1f984",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ],
    "id": "9bd1f984"
   },
   "outputs": [],
   "source": "# 将数据拆分为训练集、验证集和测试集。另外，我们还将\n# 创建一个小的开发集作为训练数据的子集；\n# 我们可以使用它来开发，这样我们的代码运行得更快。\nnum_training = 49000\nnum_validation = 1000\nnum_test = 1000\nnum_dev = 500\n\n# 我们的验证集将是从原始\n# 训练集中取 num_validation 个点。\nmask = range(num_training, num_training + num_validation)\nX_val = X_train[mask]\ny_val = y_train[mask]\n\n# 我们的训练集将是从原始\n# 训练集的前 num_train 个点。\nmask = range(num_training)\nX_train = X_train[mask]\ny_train = y_train[mask]\n\n# 我们还将制作一个开发集，它是\n# 训练集的一个小子集。\nmask = np.random.choice(num_training, num_dev, replace=False)\nX_dev = X_train[mask]\ny_dev = y_train[mask]\n\n# 我们使用原始测试集的前 num_test 个点作为我们的\n# 测试集。\nmask = range(num_test)\nX_test = X_test[mask]\ny_test = y_test[mask]\n\nprint('训练数据维度: ', X_train.shape)\nprint('训练标签维度: ', y_train.shape)\nprint('验证数据维度: ', X_val.shape)\nprint('验证标签维度: ', y_val.shape)\nprint('测试数据维度: ', X_test.shape)\nprint('测试标签维度: ', y_test.shape)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd02f9",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ],
    "id": "9bdd02f9"
   },
   "outputs": [],
   "source": "# 预处理：将图像数据重新整形为行\nX_train = np.reshape(X_train, (X_train.shape[0], -1))\nX_val = np.reshape(X_val, (X_val.shape[0], -1))\nX_test = np.reshape(X_test, (X_test.shape[0], -1))\nX_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n\n# 作为健全性检查，打印数据的维度\nprint('训练数据维度: ', X_train.shape)\nprint('验证数据维度: ', X_val.shape)\nprint('测试数据维度: ', X_test.shape)\nprint('开发数据维度: ', X_dev.shape)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066fffc7",
   "metadata": {
    "tags": [
     "pdf-ignore-input"
    ],
    "id": "066fffc7"
   },
   "outputs": [],
   "source": "# 预处理：减去平均图像\n# 首先：基于训练数据计算图像均值\nmean_image = np.mean(X_train, axis=0)\nprint(mean_image[:10]) # 打印几个元素\nplt.figure(figsize=(4,4))\nplt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) # 可视化平均图像\nplt.show()\n\n# 第二：从训练和测试数据中减去平均图像\nX_train -= mean_image\nX_val -= mean_image\nX_test -= mean_image\nX_dev -= mean_image\n\n# 第三：添加偏置维度（偏置技巧）以便我们的分类器\n# 只需要担心优化一个权重矩阵 W。\nX_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\nX_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\nX_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\nX_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n\nprint(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"
  },
  {
   "cell_type": "markdown",
   "id": "605c22f8",
   "metadata": {
    "id": "605c22f8"
   },
   "source": "## Softmax 分类器\n\n你为本节编写的代码都将写在 `cs231n/classifiers/softmax.py` 中。\n\n如你所见，我们已经预填充了 `softmax_loss_naive` 函数，它使用 for 循环来评估 softmax 损失函数。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56073f14",
   "metadata": {
    "id": "56073f14"
   },
   "outputs": [],
   "source": "# 评估我们为你提供的朴素实现：\nfrom cs231n.classifiers.softmax import softmax_loss_naive\nimport time\n\n# 生成一个由小数组成的随机 Softmax 分类器权重矩阵\nW = np.random.randn(3073, 10) * 0.0001\n\nloss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\nprint('损失: %f' % (loss, ))\n\n# 作为一个粗略的健全性检查，我们的损失应该接近 -log(0.1)。\nprint('损失: %f' % loss)\nprint('健全性检查: %f' % (-np.log(0.1)))"
  },
  {
   "cell_type": "markdown",
   "source": "**内联问题 1**\n\n为什么我们期望损失接近 -log(0.1)？简要解释。\n\n$\\color{blue}{\\textit 你的答案：}$ *在此填写*",
   "metadata": {
    "id": "NT_PDkFhZlMy"
   },
   "id": "NT_PDkFhZlMy"
  },
  {
   "cell_type": "markdown",
   "id": "7dad4937",
   "metadata": {
    "id": "7dad4937"
   },
   "source": "上面函数返回的 `grad` 现在全是零。推导并实现 softmax 损失函数的梯度，并在 `softmax_loss_naive` 函数内部内联实现它。你会发现将新代码交织在现有函数中很有帮助。\n\n为了检查你正确实现了梯度，你可以数值估计损失函数的梯度，并与你计算的梯度进行比较。我们为你提供了执行此操作的代码："
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d5ad22",
   "metadata": {
    "id": "f7d5ad22"
   },
   "outputs": [],
   "source": "# 一旦你实现了梯度，使用下面的代码重新计算它\n# 并使用我们提供的函数进行梯度检查\n\n# 计算在 W 处的损失及其梯度。\nloss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n\n# 在几个随机选择的维度上数值计算梯度，并\n# 将它们与你解析计算的梯度进行比较。数字应该在\n# 所有维度上几乎完全匹配。\nfrom cs231n.gradient_check import grad_check_sparse\nf = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\ngrad_numerical = grad_check_sparse(f, W, grad)\n\n# 在开启正则化的情况下再次进行梯度检查\n# 你没有忘记正则化梯度吧？\nloss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\nf = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\ngrad_numerical = grad_check_sparse(f, W, grad)"
  },
  {
   "cell_type": "markdown",
   "id": "665fc684",
   "metadata": {
    "tags": [
     "pdf-inline"
    ],
    "id": "665fc684"
   },
   "source": "**内联问题 2**\n\n虽然 gradcheck 对于可靠的 softmax 损失，但有时对于 SVM 损失，gradcheck 中偶尔会有一个维度不完全匹配。造成这种差异的原因是什么？值得担心吗？在什么情况下 SVM 损失梯度检查可能失败的简单例子是什么？改变边距如何影响这种情况发生的频率？\n\n请注意，对于样本 $(x_i, y_i)$，SVM 损失定义为：$$L_i = \\sum_{j\\ne y_i}\\max(0, s_j - s_{y_i} + \\Delta)$$ 其中 $j$ 遍历除正确类别 $y_i$ 外的所有类别，$s_j$ 表示第 $j$ 个类别的分类器分数。$\\Delta$ 是一个标量边距。有关更多信息，请参阅[此](https://cs231n.github.io/linear-classify/)页面上的\"多类支持向量机损失\"。\n\n*提示：SVM 损失函数严格来说不是可微的。*\n\n$\\color{blue}{\\textit 你的答案：}$ *在此填写*。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310ca148",
   "metadata": {
    "scrolled": true,
    "test": "vectorized_time_1",
    "id": "310ca148"
   },
   "outputs": [],
   "source": "# 接下来实现函数 softmax_loss_vectorized；现在只计算损失；\n# 我们稍后会实现梯度。\ntic = time.time()\nloss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\ntoc = time.time()\nprint('朴素损失: %e 用时 %fs' % (loss_naive, toc - tic))\n\nfrom cs231n.classifiers.softmax import softmax_loss_vectorized\ntic = time.time()\nloss_vectorized, _ = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\ntoc = time.time()\nprint('向量化损失: %e 用时 %fs' % (loss_vectorized, toc - tic))\n\n# 损失应该匹配，但你的向量化实现应该快得多。\nprint('差异: %f' % (loss_naive - loss_vectorized))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5d3643",
   "metadata": {
    "test": "vectorized_time_2",
    "id": "ef5d3643"
   },
   "outputs": [],
   "source": "# 完成 softmax_loss_vectorized 的实现，并以向量化的方式计算损失函数的梯度。\n\n# 朴素实现和向量化实现应该匹配，但\n# 向量化版本仍然应该快得多。\ntic = time.time()\n_, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\ntoc = time.time()\nprint('朴素损失和梯度: 用时 %fs' % (toc - tic))\n\ntic = time.time()\n_, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\ntoc = time.time()\nprint('向量化损失和梯度: 用时 %fs' % (toc - tic))\n\n# 损失是一个数字，所以很容易比较\n# 两种实现计算的值。另一方面，梯度是一个矩阵，所以\n# 我们使用 Frobenius 范数来比较它们。\ndifference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\nprint('差异: %f' % difference)"
  },
  {
   "cell_type": "markdown",
   "id": "635eef89",
   "metadata": {
    "id": "635eef89"
   },
   "source": "### 随机梯度下降\n\n现在我们有了向量化且高效的损失表达式、梯度，并且我们的梯度与数值梯度匹配。因此我们准备进行 SGD 以最小化损失。你为本节编写的代码将写在 `cs231n/classifiers/linear_classifier.py` 中。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a13952",
   "metadata": {
    "test": "sgd",
    "id": "a5a13952"
   },
   "outputs": [],
   "source": "# 在文件 linear_classifier.py 中，在函数\n# LinearClassifier.train() 中实现 SGD，然后运行下面的代码。\nfrom cs231n.classifiers import Softmax\nsoftmax = Softmax()\ntic = time.time()\nloss_hist = softmax.train(X_train, y_train, learning_rate=1e-7, reg=2.5e4,\n                      num_iters=1500, verbose=True)\ntoc = time.time()\nprint('用时 %fs' % (toc - tic))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abad0c45",
   "metadata": {
    "id": "abad0c45"
   },
   "outputs": [],
   "source": "# 一个有用的调试策略是将损失绘制为\n# 迭代次数的函数：\nplt.plot(loss_hist)\nplt.xlabel('迭代次数')\nplt.ylabel('损失值')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54af4f",
   "metadata": {
    "test": "validate",
    "id": "2e54af4f"
   },
   "outputs": [],
   "source": "# 编写 LinearClassifier.predict 函数并在\n# 训练集和验证集上评估性能\n# 你应该得到大约 0.34 的验证准确率（> 0.33）。\ny_train_pred = softmax.predict(X_train)\nprint('训练准确率: %f' % (np.mean(y_train == y_train_pred), ))\ny_val_pred = softmax.predict(X_val)\nprint('验证准确率: %f' % (np.mean(y_val == y_val_pred), ))"
  },
  {
   "cell_type": "code",
   "source": "# 保存训练好的模型用于自动评分。\nsoftmax.save(\"softmax.npy\")",
   "metadata": {
    "id": "O4ac8brXSxeo"
   },
   "id": "O4ac8brXSxeo",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce2da33",
   "metadata": {
    "tags": [
     "code"
    ],
    "test": "tuning",
    "id": "1ce2da33"
   },
   "outputs": [],
   "source": "# 使用验证集来调整超参数（正则化强度和学习率）。\n# 你应该尝试不同的学习率和正则化强度范围；如果你仔细调整，\n# 你应该能在验证集上获得大约 0.365（> 0.36）的分类准确率。\n\n# 注意：在超参数搜索期间，你可能会看到运行时/溢出警告。\n# 这可能是由极端值引起的，并不是错误。\n\n# results 是一个字典，将形式为\n# (learning_rate, regularization_strength) 的元组映射到\n# 形式为 (training_accuracy, validation_accuracy) 的元组。准确率简单地\n# 是被正确分类的数据点的分数。\nresults = {}\nbest_val = -1   # 到目前为止我们看到的最高验证准确率。\nbest_softmax = None # 达到最高验证率的 Softmax 对象。\n\n################################################################################\n# TODO:                                                                        #\n# 编写在验证集上调整超参数的代码。对于每组超参数组合，训练一个 Softmax，#\n# 在训练集上训练，在训练集和验证集上计算其准确率，并将这些数字存储在     #\n# results 字典中。此外，将最佳验证准确率存储在 best_val 中，实现这一点的   #\n# Softmax 对象存储在 best_softmax 中。                                        #\n#                                                                              #\n# 提示：在开发验证代码时，你应该使用较小的 num_iters 值，这样分类器就不会花很多时间训练；一旦你确信验证代码有效，你应该用较大的 num_iters 值重新运行代码。 #\n################################################################################\n\n# 作为参考提供。你可能想也可能不想改变这些超参数\nlearning_rates = [1e-7, 1e-6]\nregularization_strengths = [2.5e4, 1e4]\n\n\n\n# 打印结果。\nfor lr, reg in sorted(results):\n    train_accuracy, val_accuracy = results[(lr, reg)]\n    print('lr %e reg %e 训练准确率: %f 验证准确率: %f' % (\n                lr, reg, train_accuracy, val_accuracy))\n\nprint('交叉验证期间达到的最佳验证准确率: %f' % best_val)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b520723",
   "metadata": {
    "tags": [
     "pdf-ignore-input"
    ],
    "id": "1b520723"
   },
   "outputs": [],
   "source": "# 可视化交叉验证结果\nimport math\nimport pdb\n\n# pdb.set_trace()\n\nx_scatter = [math.log10(x[0]) for x in results]\ny_scatter = [math.log10(x[1]) for x in results]\n\n# 绘制训练准确率\nmarker_size = 100\ncolors = [results[x][0] for x in results]\nplt.subplot(2, 1, 1)\nplt.tight_layout(pad=3)\nplt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap=plt.cm.coolwarm)\nplt.colorbar()\nplt.xlabel('log 学习率')\nplt.ylabel('log 正则化强度')\nplt.title('CIFAR-10 训练准确率')\n\n# 绘制验证准确率\ncolors = [results[x][1] for x in results] # 默认标记大小为 20\nplt.subplot(2, 1, 2)\nplt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap=plt.cm.coolwarm)\nplt.colorbar()\nplt.xlabel('log 学习率')\nplt.ylabel('log 正则化强度')\nplt.title('CIFAR-10 验证准确率')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b29768e",
   "metadata": {
    "test": "test",
    "id": "7b29768e"
   },
   "outputs": [],
   "source": "# 在测试集上评估最佳 softmax\ny_test_pred = best_softmax.predict(X_test)\ntest_accuracy = np.mean(y_test == y_test_pred)\nprint('在原始像素上的 Softmax 分类器最终测试集准确率: %f' % test_accuracy)"
  },
  {
   "cell_type": "code",
   "source": "# 保存最佳 softmax 模型\nbest_softmax.save(\"best_softmax.npy\")",
   "metadata": {
    "id": "JU4xmsWpUIAd"
   },
   "id": "JU4xmsWpUIAd",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e122c35",
   "metadata": {
    "tags": [
     "pdf-ignore-input"
    ],
    "id": "9e122c35"
   },
   "outputs": [],
   "source": "# 可视化每个类别学到的权重。\n# 根据你选择的学习率和正则化强度，这些可能\n# 看起来不错也可能不怎么样。\nw = best_softmax.W[:-1,:] # 去掉偏置\nw = w.reshape(32, 32, 3, 10)\nw_min, w_max = np.min(w), np.max(w)\nclasses = ['飞机', '汽车', '鸟', '猫', '鹿', '狗', '青蛙', '马', '船', '卡车']\nfor i in range(10):\n    plt.subplot(2, 5, i + 1)\n\n    # 将权重重新缩放到 0 到 255 之间\n    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n    plt.imshow(wimg.astype('uint8'))\n    plt.axis('off')\n    plt.title(classes[i])"
  },
  {
   "cell_type": "markdown",
   "id": "44b827ad",
   "metadata": {
    "tags": [
     "pdf-inline"
    ],
    "id": "44b827ad"
   },
   "source": "**内联问题 3**\n\n描述你可视化的 Softmax 分类器权重是什么样的，并简要解释为什么它们看起来是这样的。\n\n$\\color{blue}{\\textit 你的答案：}$ *在此填写*"
  },
  {
   "cell_type": "markdown",
   "source": "**内联问题 4** - *正确或错误*\n\n假设总体训练损失定义为所有训练示例上逐数据点损失的总和。可以向训练集添加一个新的数据点，该数据点会改变 softmax 损失，但使 SVM 损失保持不变。\n\n$\\color{blue}{\\textit 你的答案：}$\n\n\n$\\color{blue}{\\textit 你的解释：}$",
   "metadata": {
    "id": "DvBKlibgaiy9"
   },
   "id": "DvBKlibgaiy9"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "_gLYWFm7akSI"
   },
   "id": "_gLYWFm7akSI",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}